** What are retrievers **

A retriever is a component in LangChain that fetches relevant documents from a data source in response to user query

there are multiple types of retrievers. All retrievers in LangChain are runnables.

** Types of Retrievers **

Retrievers can be categorized into two types:

1) Based on the DataSource
     - Wikipedia Retrievers [Get the relevant Wikipedia articles based on the user query]
     - Vector Store
     - Archieve Retriver - Get the relevant research documents base on the user query
2) Based on Retriver Search Category
     - MMR (Maximum Marginal Relevance)
     - Multi-Query Retriver
     - Contextual Compression retriever


** Wikipedia Retriver **
A Wikipedia retriever is a retriever that queries the wikipedia API to fetch relevant content for a given query

How it works:
1) You give it a query (e.g., Machine Learning)
2) It sends the query to Wikipedia API
3) It retrives the most relevant articles
4) It retuns them as LangChain Document Objects

** Vector Store Retriever **

A vector store retriever in LangChain is the most common type of retriever that lets you search and fetch documents from a vector store based on semantic similarity using vector embeddings

How it works:
1) you store your documents in a vector store (like FAISS, Chroma, Weaviate)
2) Each document is converted into a dense vector using an embeddings model
3) When the user enters a query:
       - It's also turned into a vector
       - The retriever compares the query vector with the stored vectors
       - it retrieves the top-k most similar ones

** Maximal Marginal Relevance(MMR) **

" How can we pick results that are not only relevant to the query but also different from each other"
MMR is an information retrievel algorithem designed to reduce redundancy in the retrieved results while maintaining high relevance to the query.

Why MMR retreiever?

In regular similarity search you may get documents that are 
 - all very similar to each other
 - Repeating the same information
 - Lacking Diverse perspectives

MMR retriver avoids that by:
 - picking the most reelvant documents first
 - Then picking the next most relevant and least similar to already selected documents
 - And so on..

 This helps especially in RAG pipelines where:
 - you want your context window to contain diverse but still relevant information
 -Expecially usedul when documents are semantically overlapping


 ** Multi Query Retriever **

 Sometimes a single query might not capture all the ways information is phrased in your documents

 Example: Query: How can i stay Heathy?
 Could mean

  - What should i eat?
  - How often should i execrcise?
  - How can i manage stress?

  A simple similarity search might miss documents that talk about those things but don't use the word healthy.

  How this works?
   - Takes your query
   - Uses an LLM - to get multiple semantically different versions of that query
   - perform retrieval for each sub query
   - combines and deduplicates the results 

** Contextual Compression retreiever **

The Contextual compression retriever in LangChain is an advanced retriever that improves retrieval quality by compressing documents after retrieval - keeping only the relavent content based on the user's query.

Query:

what is Photosysnthesis?

Retrieved Document by traditional retreiver:
The Grand canyon is a famous natural site. Photosysnthesis is how plants convert light into energy. Many tourists visist every year.

Problem:
The retriever returned the entire paragraph. Only one sentence is actaullay relevant to the query. The rest is irrevalent noise that wastes context window and may confuse the LLM.

What contexttual compression retreiver does:
Returns only relavent part - Photosysnthesis is how plants convert light into energy

How it works:
1) Base retriever(e.h., FAISS, Chroma) retrieves N documents
2) A compressor (Usually an LLM) is applied to each document
3) The compressor keeps only the parts relevant to the query
4) irrevalent content is discarded

When to use:
1) Your documents are long and contain mixed information
2) You want to reduce context length in LLMs
3) You need to improve answer accuracy in RAG pipelines