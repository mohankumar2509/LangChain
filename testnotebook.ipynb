{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a752ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\MyCompany\\MachineLearning\\GenAI\\LangChain\\mohan\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"HF_HOME\"] =\"W:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a132d",
   "metadata": {},
   "source": [
    "## deepseek-ai/deepseek-coder-1.3b-instruct\n",
    "\n",
    "### Model with Just Computer Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a08e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\MyCompany\\MachineLearning\\GenAI\\LangChain\\mohan\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mohan\\.cache\\huggingface\\hub\\models--deepseek-ai--deepseek-coder-1.3b-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\nWhat is the capital of India?\\n### Response:\\nI'm sorry, but as an AI focused on computer science, I don't provide information on geographical questions. I recommend using a search engine or a geography-focused resource for this information.\\n\", additional_kwargs={}, response_metadata={}, id='run--87480c02-737e-4b33-867d-f78696f69f44-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm =HuggingFacePipeline.from_model_id(model_id=\"deepseek-ai/deepseek-coder-1.3b-instruct\", task = \"text-generation\")\n",
    "model = ChatHuggingFace(llm = llm)\n",
    "model.invoke(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdaa4809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM stands for Language Model. It's a part of the HuggingFace library. It's a type of language model that allows for text generation and modification. Here is an example of how to use LLM in Python using the Langchain library:\n",
      "\n",
      "Firstly, make sure you have installed the necessary libraries. You can install them using pip:\n",
      "\n",
      "```bash\n",
      "pip install transformers\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "Then, you can use the following Python code:\n",
      "\n",
      "```python\n",
      "from langchain.llms.openai import OpenAIModel\n",
      "from langchain.llms.base import LLM\n",
      "from langchain.prompts.base import BasePrompt\n",
      "from langchain.prompts.chat import ChatPrompt\n",
      "\n",
      "# Define your prompt\n",
      "prompt = BasePrompt.create(\n",
      "    prompt=ChatPrompt.from_user(\"Hello World!\")\n",
      ")\n",
      "\n",
      "# Create a language model\n",
      "model = LLM(OpenAIModel(model_name=\"text-davinci-003\"))\n",
      "\n",
      "# Use the model to generate a response\n",
      "response = model.predict(prompt)\n",
      "\n",
      "# Print the response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write python code with langchain to use LLM?\"\n",
    "response = model.invoke(prompt)\n",
    "print(response.content.split(\"Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb1cd4",
   "metadata": {},
   "source": [
    "## TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "\n",
    "### text-generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d9c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Electins, also known as lectins, are a family of proteins that are highly conserved throughout the animal kingdom. They are found in the extracellular matrix (ECM) of various tissues and play a crucial role in maintaining tissue integrity, cell adhesion, and immune surveillance. Electins are produced by various types of cells, including endothelial cells, macrophages, and basophils, and they bind to specific proteins on the surface of antigen-presenting cells (APCs), activating the immune response.\n",
      "\n",
      "In India, electins have been extensively studied due to their potential applications in various fields, including medical research, pharmaceuticals, and agriculture. Electins have been found to play a crucial role in immune surveillance, with genes involved in the production and secretion of electins being associated with various immune-related diseases such as autoimmune disorders, allergy, and cancer.\n",
      "\n",
      "In addition, electins have been used in various medical research fields, such as cancer therapy, diagnostics, and drug delivery. Electins have been found to bind to cancer cells, leading to their detachment from their supportive tissues and subsequent elimination by the immune system. In diagnostics, electins have been used to detect specific types of diseases, such as tuberculosis and malaria, by analyzing the serum or urine samples.\n",
      "\n",
      "Lastly, electins have been used in agriculture to improve crop yields and reduce crop loss due to pests and diseases. Several studies have shown that electins produced by different plant species can be used as a natural pesticide or biocontrol agent. In addition, electins can be used to develop new plant-based ingredients with anti-allergenic properties.\n",
      "\n",
      "In conclusion, electins play an essential role in the immune system, tissue biology, medical research, and agriculture in India. By studying electins, their production, secretion, and interactions with APCs, researchers can continue to advance our understanding of biology, improve healthcare, and enhance agriculture.\n"
     ]
    }
   ],
   "source": [
    "llm =HuggingFacePipeline.from_model_id(model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", task = \"text-generation\")\n",
    "model = ChatHuggingFace(llm = llm)\n",
    "result= model.invoke(\"Explain how electins will be conducted in india\")\n",
    "print(result.content.split(\"<|assistant|>\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d62b2",
   "metadata": {},
   "source": [
    "## google/gemma-3-270m-it\n",
    "\n",
    "### for text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2371f871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Electins are a fascinating and versatile material with a wide range of applications in various fields. They are a fascinating material with a unique combination of properties that make them suitable for a variety of applications. Here's a breakdown of how electins will be conducted in India:\n",
      "\n",
      "**1. Production and Manufacturing:**\n",
      "\n",
      "*   **Raw Material:** Electins are primarily derived from various materials, including:\n",
      "    *   **Polymer:** They are synthesized from polymers like polyethylene, polypropylene, and polystyrene.\n",
      "    *   **Silica:** Silica is a common and cost-effective raw material.\n",
      "    *   **Glass:** Glass\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"google/gemma-3-270m-it\",\n",
    "    token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    device=-1,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "\n",
    "llm =HuggingFacePipeline(pipeline=generator)\n",
    "model = ChatHuggingFace(llm = llm)\n",
    "result= model.invoke(\"Explain how electins will be conducted in india\")\n",
    "print(result.content.split(\"<start_of_turn>model\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7d3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.26it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"), task= \"text-generation\", model = \"mistralai/Mistral-7B-Instruct-v0.3\", device=-1, max_new_tokens=500)\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "model = ChatHuggingFace(llm = llm)\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "print(result.content.split(\"[/INST]\")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ce9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mohan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
